---
title: SEO Issues - All Causes & Fixes
description: Diagnose and fix Lighthouse SEO failures. Every cause of poor SEO scores with step-by-step solutions for crawlability and indexing issues.
navigation:
  title: All SEO Issues
icon: i-heroicons-wrench-screwdriver
publishedAt: 2025-01-18
updatedAt: 2025-01-18
keywords:
  - fix seo lighthouse
  - seo issues
  - crawlability
  - indexing problems
tags:
  - seo
  - lighthouse
  - troubleshooting
---

# SEO Issues & Fixes

Not sure why your SEO audit is failing? Every Lighthouse SEO audit is pass/fail, so failures are usually specific and fixable.

The good news: most SEO failures come from a handful of causes. Find yours below.

## Quick Diagnosis

Run these checks before diving into specific fixes:

**1. Check if the page is indexable:**
```bash
curl -I https://yoursite.com/page | grep -i "x-robots"
```
Any `noindex` in the response? That's your problem.

**2. View page source for meta robots:**
```html
<meta name="robots" content="noindex">
```
Search for "noindex" in your page source.

**3. Check robots.txt:**
```bash
curl https://yoursite.com/robots.txt
```
Is your page's path disallowed?

**4. Verify canonical:**
Search page source for `rel="canonical"`. Is the URL correct? Is there only one?

**5. Check HTTP status:**
```bash
curl -o /dev/null -s -w "%{http_code}" https://yoursite.com/page
```
Anything other than 2xx is a problem.

## All SEO Causes

### Blocking Issues (Fix These First)

These prevent your page from being indexed entirely.

#### [Page is blocked from indexing](/learn-lighthouse/seo/fix/is-crawlable)

Search engines can't include your pages in results if you've blocked them. Common causes:
- `<meta name="robots" content="noindex">` in HTML
- `X-Robots-Tag: noindex` HTTP header
- `Disallow` in robots.txt for Googlebot

This is often intentional (staging sites, user dashboards) but sometimes accidental (dev config leaked to prod, CMS default settings).

→ Remove blocking directives or ensure they're intentional

#### [Page has unsuccessful HTTP status code](/learn-lighthouse/seo/fix/http-status-code)

Pages returning 4xx or 5xx errors aren't indexed. Lighthouse flags any response code from 400-599.

Common causes:
- Broken links pointing to deleted pages
- Incorrect redirects
- Server errors under load
- Auth walls returning 401/403

→ Fix the underlying error or redirect to a valid page

#### [robots.txt is not valid](/learn-lighthouse/seo/fix/robots-txt)

Malformed robots.txt can accidentally block your entire site. Lighthouse validates syntax including:
- Invalid directives
- Missing user-agent before allow/disallow
- Invalid sitemap URLs
- Syntax errors

→ Validate and fix robots.txt syntax

### Canonicalization Issues

These confuse Google about which URL to index.

#### [Document does not have a valid rel=canonical](/learn-lighthouse/seo/fix/canonical)

Invalid canonicals send wrong signals to search engines. Failures include:
- Multiple conflicting canonical URLs
- Relative instead of absolute URLs
- Canonical pointing to a different hreflang page
- Canonical always pointing to homepage (common mistake)

→ Ensure one valid absolute canonical URL per page

### Content Discovery Issues

These affect how Google understands your content and links.

#### [Links are not crawlable](/learn-lighthouse/seo/fix/crawlable-anchors)

Anchor elements without valid hrefs block crawlers from discovering content. Failures include:
- `href="javascript:void(0)"`
- Empty href attributes
- `href="#"` with onclick handlers
- Malformed URLs

→ Use real URLs for navigation links, use buttons for actions

#### [Links do not have descriptive text](/learn-lighthouse/seo/fix/link-text)

Generic link text hurts SEO signals. Flagged phrases include:
- "click here"
- "read more"
- "learn more"
- "here"
- "more info"

These tell Google nothing about the destination page.

→ Use descriptive anchor text: "read our pricing guide" not "click here"

### Metadata Issues

These affect search result appearance and click-through rates.

#### [Document does not have a meta description](/learn-lighthouse/seo/fix/meta-description)

Missing meta descriptions mean Google generates snippets automatically—often pulling random text from your page.

The description doesn't directly affect ranking but significantly impacts click-through rates in search results.

→ Add unique, compelling meta descriptions to every page

### International SEO Issues

Only relevant for multi-language/region sites.

#### [Document doesn't have a valid hreflang](/learn-lighthouse/seo/fix/hreflang)

Invalid hreflang breaks international targeting. Failures include:
- Invalid or misspelled language codes
- Relative instead of absolute URLs
- Missing self-referencing hreflang
- Incomplete hreflang sets

→ Use valid ISO language codes with fully-qualified URLs

## Decision Tree

Work through this to find your issue:

1. **Is the page blocked?**
   - Check for `noindex` in meta/headers → [Fix crawlability](/learn-lighthouse/seo/fix/is-crawlable)
   - Check robots.txt for disallow → [Fix robots.txt](/learn-lighthouse/seo/fix/robots-txt)

2. **Does the page return 200?**
   - 4xx or 5xx? → [Fix HTTP status](/learn-lighthouse/seo/fix/http-status-code)

3. **Is the canonical correct?**
   - Missing, multiple, or invalid? → [Fix canonical](/learn-lighthouse/seo/fix/canonical)

4. **Check on-page elements:**
   - Missing meta description? → [Add meta description](/learn-lighthouse/seo/fix/meta-description)
   - Generic link text? → [Fix link text](/learn-lighthouse/seo/fix/link-text)
   - JavaScript links? → [Fix crawlable links](/learn-lighthouse/seo/fix/crawlable-anchors)

5. **Multi-language site?**
   - Hreflang issues? → [Fix hreflang](/learn-lighthouse/seo/fix/hreflang)

## Common Patterns by Framework

| Framework | Typical SEO Issues | Fix Pattern |
|-----------|-------------------|-------------|
| **Next.js** | Metadata component missing, dynamic routes without canonical | Use `generateMetadata`, set canonical in layout |
| **Nuxt** | `useSeoMeta` not called, trailing slash inconsistency | Add SEO module, configure trailing slash |
| **WordPress** | Plugin conflicts, auto-generated descriptions | Use Yoast/RankMath consistently |
| **Shopify** | Variant pages blocking, thin descriptions | Configure theme settings |
| **React SPA** | No SSR = no content for crawlers | Add SSR or prerendering |
| **Angular** | Universal SSR complexity | Ensure SSR includes meta tags |

## Test Every Page

SEO issues often hide in specific templates:
- Blog post template missing meta descriptions
- Product pages with wrong canonicals
- Category pagination blocking robots
- Old URLs returning soft 404s

Testing your homepage tells you nothing about these.

[Unlighthouse](/cloud) runs SEO audits across your entire site. You'll see exactly which pages fail each audit, which templates are broken, and catch issues before they impact rankings.

The CLI runs locally for free. Cloud adds monitoring to catch regressions from deploys.

## Related

- [What are SEO Audits?](/learn-lighthouse/seo)
- [Core Web Vitals Overview](/learn-lighthouse/core-web-vitals)
- [Bulk Lighthouse Testing](/learn-lighthouse/bulk-lighthouse-testing)
